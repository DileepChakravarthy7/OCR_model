# -*- coding: utf-8 -*-
"""HW_recog_v3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P3EF6plT_BLVm7AfIPAY7j3RkCuCukMf
"""

# import the necessary packages
from tensorflow.keras.models import load_model
from imutils.contours import sort_contours
import numpy as np
import imutils
import cv2

# load the handwriting OCR model
print("[INFO] loading handwriting OCR model...")
model = load_model('/content/drive/MyDrive/handwritten (96.5%).h5')

# load the input image from disk, convert it to grayscale, and blur
# it to reduce noise
image = cv2.imread("/content/drive/MyDrive/ocr_test_images/Failed_2.JPG", cv2.IMREAD_COLOR)
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
blurred = cv2.GaussianBlur(gray, (5,5), 0)
# perform edge detection, find contours in the edge map, and sort the
# resulting contours from left-to-right
edged = cv2.Canny(blurred, 30, 230)
cnts = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL,
	cv2.CHAIN_APPROX_SIMPLE)

cnts = imutils.grab_contours(cnts)
cnts = sort_contours(cnts, method="left-to-right")[0]

# initialize the list of contour bounding boxes and associated
# characters that we'll be OCR'ing
chars_1 = []

from google.colab.patches import cv2_imshow
cv2_imshow(edged.copy())
cv2_imshow(blurred)
cv2.waitKey(0)

# loop over the contours
for c in cnts:
	# compute the bounding box of the contour
	rect = cv2.boundingRect(c)
	x, y, w, h = rect
	if rect[1]>=50:
		# filter out bounding boxes, ensuring they are neither too small
		# nor too large
		if (w >= 5 and w <= 150) and (h >= 15 and h <= 120):
			# extract the character and threshold it to make the character
			# appear as *white* (foreground) on a *black* background, then
			# grab the width and height of the thresholded image
			roi = gray[y:y + h, x:x + w]
			thresh = cv2.threshold(roi, 0, 255,
				cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]
			(tH, tW) = thresh.shape
			# if the width is greater than the height, resize along the
			# width dimension
			if tW > tH:
				thresh = imutils.resize(thresh, width=32)
			# otherwise, resize along the height
			else:
				thresh = imutils.resize(thresh, height=32)
   		# re-grab the image dimensions (now that its been resized)
			# and then determine how much we need to pad the width and
			# height such that our image will be 32x32
			(tH, tW) = thresh.shape
			dX = int(max(0, 32 - tW) / 2.0)
			dY = int(max(0, 32 - tH) / 2.0)
			# pad the image and force 32x32 dimensions
			padded = cv2.copyMakeBorder(thresh, top=dY, bottom=dY,
				left=dX, right=dX, borderType=cv2.BORDER_CONSTANT,
				value=(0, 0, 0))
			padded = cv2.resize(padded, (32, 32))
			# prepare the padded image for classification via our
			# handwriting OCR model
			padded = padded.astype("float32") / 255.0
			padded = np.expand_dims(padded, axis=-1)
			# update our list of characters that will be OCR'd
			chars_1.append((padded, (x, y, w, h)))
  	# extract the bounding box locations and padded characters
	
boxes = [b[1] for b in chars_1]
chars_1 = np.array([c[0] for c in chars_1], dtype="float32")

print(len(chars_1))

import matplotlib.pyplot as plt
fig = plt.figure(figsize=(10, 7))
rows = 5
columns = 4
a=chars_1[0].reshape(32,32)
b=chars_1[1].reshape(32,32)
c=chars_1[2].reshape(32,32)
d=chars_1[3].reshape(32,32)
e=chars_1[4].reshape(32,32)
#f=chars_1[5].reshape(32,32)
#g=chars_1[6].reshape(32,32)
#h=chars_1[7].reshape(32,32)

fig.add_subplot(rows, columns, 1)
plt.imshow(a)
plt.axis('off')
plt.title("char_1")

fig.add_subplot(rows, columns, 2)
plt.imshow(b)
plt.axis('off')
plt.title("char_2")

fig.add_subplot(rows, columns, 3)
plt.imshow(c)
plt.axis('off')
plt.title("char_3")

fig.add_subplot(rows, columns, 4)
plt.imshow(d)
plt.axis('off')
plt.title("char_4")

fig.add_subplot(rows, columns, 5)
plt.imshow(e)
plt.axis('off')
plt.title("char_5")

#fig.add_subplot(rows, columns, 6)
#plt.imshow(f)
#plt.axis('off')
#plt.title("char_6")

#fig.add_subplot(rows, columns, 7)
#plt.imshow(g)
#plt.axis('off')
#plt.title("char_7")

#fig.add_subplot(rows, columns, 8)
#plt.imshow(h)
#plt.axis('off')
#plt.title("char_8")

preds = model.predict(chars_1)
# define the list of label names
labelNames = "0123456789"
labelNames += "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
labelNames = [l for l in labelNames]

# Python program to convert a list
# of character

def convert(s):

	# initialization of string to ""
	new = ""

	# traverse in the string
	for x in s:
		new += x

	# return string
	return new

from google.colab.patches import cv2_imshow
# loop over the predictions and bounding box locations together
label_1=[]

for (pred, (x, y, w, h)) in zip(preds, boxes):
	# find the index of the label with the largest corresponding
	# probability, then extract the probability and label
	i = np.argmax(pred)
	prob = pred[i]
	label = labelNames[i]
	label_1.append(label)
	#print(label)
	# draw the prediction on the image
	print("[INFO] {} - {:.2f}%".format(label, prob * 100))
	#cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)	
cv2.putText(image,str(convert(label_1)), (x-80, y+40),cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)
# show the image

cv2_imshow(image)
cv2.waitKey(0)

print(convert(label_1))



